# Advancing Plain Vision Transformer Towards Remote Sensing Foundation Model

> Di Wang

Large-scale vision foundation models have made significant progress in visual tasks on natural images, where the vision transformers are the primary chocie for their good scalability and representation ability.

However, the utilization of large models in the remote sensing (RS) community remains under-explored where existing models are still at small-scale, which limits the performance.

In this paper, we resort to plain vision transformers with about 100 million parameters and make the first attempt to propose large vision models customized for RS tasks and explore how such large models perform.

Specifically, to handle the large image size and objects of various orientations in RS images, we propose a new rotated varied-size window attention to substitute the original full attention in transformers, which could significantly reduce the computational cost and memory footprint while learn better object representation by extracting rich context from the generated diverse windows.

Experiments on detection tasks demonstrate the superiority of our model over all state-of-the-art models, achieving 81.16% mAP on the DOTA-V1.0 dataset.

The geninformatics community is vigoriously developing by combining with computer technologies because of the necessity for human production, and the most representative achievement is the "4D product" including digital Line graphic, digital elevation model, digital raster graphic, and digital orthophoto map (DOM). Among these productions, the remote sensing image (RSI) is the dominant data source for generating the DOM since it is easy to access and can be obtained in real-time.

The RSIs are being employed in many valuable applications such as scene recognition for land use and land cover classification for precision agriculture and object detection for matitime monitoring.

However, existing studies reveal that the limited receptive field of convolution in each layer makes it difficult for CNNs to pay attention to long-range pixels and extract global context.

To address this issue, the self-attention (SA) mechanism is proposed to obtain flexible global dependency by enabling the interaction between arbitrary pixels in images, delivering promising results in the computer vision (CV) field.

Further, vision transformer adopts the design of multi-head SA (MHSA), which simulataneously implements the above provedure in multiple projected subspaces, which diversifies the extracted contexts and improves the feature representation.

The plain vision transformer (ViT) is a very simple architecture that stacks several transformer encoder blocks sequentially after the patch embedding layer, where features after each block are at the same scale.

Nevertheless, due to the strong presentation ability of the transformers, researchers question the necessity of the hierarchial design in transformers for downstream tasks.

Thanks to the development of the unsupervised learning in masked image modeling (MIM), recent work reveals that such a pretraining processs can give a good initialization for plain ViTs to achieve surprising results on various downstream tasks including object detection and pose estimation.

The main insight behine their success is that the multiscale prior can be learned from the data during the pretraining, thus making it possible to discard the hierarchical structure.

A recently proposed method named varied-size attention (VSA) addresses this issue by learning trainable scaling factors and offset to adapt the size, shape and location of windows to diverse image content, thus delivering better performance on many tasks.

(1) We demonstrate the possibility of pretraining plain ViTs with about 100 million parameters on RSIs, adapting them for downstream RS tasks, and delivering competitive performance. To our best knowledge, they are so far the largest models in the remote sening community, making a step towards the remote sensing foundation model.

(2) We introduce a learnable rotation mechanism into the vision transformer to learn varied-size windows with different orientation angles for attention calculation, which is very suitable to deal with RSIs. It promotes the extraction of rich context from the generated windows and learning better feature representation.

(3) Experimental results show that the proposed models set new state-of-the-art (SOTA) on the detection task, and obtain competitive performances on classification and segmentation tasks. Besides, we also show the advantanges of the proposed models in terms of computational complexity and few-shot learning ability.

--------------

Plain Vision Transformerä¸­ä½¿ç”¨å…·æœ‰çº¦1äº¿ä¸ªå‚æ•°ğŸ¤¦â€â™€ï¸

Plain Vision Transformeræå‡ºäº†ä¸€ç§æ–°çš„æ—‹è½¬å¯å˜å¤§å°çª—å£æ³¨æ„rotated variedsize window attentionï¼Œæ¥å¯¹åŸæ¥çš„transformerä¸­çš„original full attentionè¿›è¡Œæ›¿æ¢ï¼Œæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬å’Œå†…å­˜å ç”¨ï¼›åŒæ—¶é€šè¿‡ä»ç”Ÿæˆçš„ä¸åŒçª—å£ä¸­æå–ä¸°å¯Œçš„ä¸Šä¸‹æ–‡æ¥å­¦ä¹ æ›´å¥½çš„å¯¹è±¡è¡¨ç¤º

|Model|mAP|Conference|Recommond|
|--|--|--|--|
|ViT+RVSA-ORCN|81.16|-|-|

--------------

RSI = remote sensing image

å½“å‰ï¼Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ˜¯æå–åˆ†å±‚å¤šå°ºåº¦è§†è§‰ç‰¹å¾æœ€å¸¸ç”¨çš„æ¨¡å‹ï¼Œæ¯å±‚å·ç§¯çš„æœ‰é™æ„Ÿå—åŸŸä½¿å¾—CNNå¾ˆéš¾å…³æ³¨è¿œè·ç¦»åƒç´ å¹¶æå–å…¨å±€ä¸Šä¸‹æ–‡ã€‚

ä¸ºäº†è§£å†³ä¸Šé¢é‚£ä¸ªé—®é¢˜ï¼Œæå‡ºäº†self-attention SA æœºåˆ¶å®ç°å›¾åƒä¸­ä»»æ„åƒç´ ä¹‹é—´çš„äº¤äº’æ¥è·å¾—çµæ´»çš„å…¨å±€ä¾èµ–æ€§

å› æ­¤ï¼Œtransformer é‡‡ç”¨äº†å¤šå¤´SAï¼ˆMHSAï¼‰çš„è®¾è®¡ï¼Œè¯¥è®¾è®¡åœ¨å¤šä¸ªæŠ•å½±å­ç©ºé—´ä¸­åŒæ—¶å®ç°äº†ä¸Šè¿°è¿‡ç¨‹ï¼Œä»è€Œä½¿æå–çš„ä¸Šä¸‹æ–‡å¤šæ ·åŒ–ï¼Œå¹¶æ”¹è¿›äº†ç‰¹å¾è¡¨ç¤ºã€‚ğŸ‘

Plain Vision Transformer = ViT åœ¨patch embeddingå±‚ä¹‹åé¡ºåºå †å å¤šä¸ªtransformerç¼–ç å™¨å—ï¼Œå…¶ä¸­æ¯ä¸ªå—ä¹‹åçš„ç‰¹å¾å…·æœ‰ç›¸åŒçš„æ¯”ä¾‹

å€Ÿé‰´äº†CNNä¸­çš„åˆ†å±‚è®¾è®¡æ€æƒ³ï¼Œå¹¶ç›¸åº”åœ°è®¾è®¡äº†åˆ†å±‚ViTã€‚è¿™äº›æ¨¡å‹é€šå¸¸ä½¿ç”¨å¤§è§„æ¨¡æ•°æ®é›†ä»¥æœ‰ç›‘ç£çš„æ–¹å¼è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶å¯¹ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå¾®è°ƒ

æœ€è¿‘çš„è®ºæ–‡æœ‰è®ºè¯ï¼Œåˆ†å±‚ViTç›¸å¯¹äºCNNçš„ä¼˜è¶Šæ€§ã€‚è€Œtransformeræœ‰å¼ºå¤§çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œé‚£ä¹ˆä¸‹æ¸¸ä»»åŠ¡ä¸­åˆ†å±‚transformerçš„è®¾è®¡æ˜¯ä¸æ˜¯å¿…è¦çš„ï¼ŸğŸ˜’

- æ ¹æ®æœ€è¿‘çš„è®ºæ–‡MiM(Masked image modeling)ä¸­æ— ç›‘ç£å­¦ä¹ çš„å‘å±•ï¼Œè¿™ç§ä¸è®­ç»ƒè¿‡ç¨‹å¯ä»¥ä¸ºplain ViTæä¾›å¾ˆå¥½çš„åˆå§‹åŒ–ï¼Œå¯¼è‡´ä¸‹æ¸¸ä»»åŠ¡çš„æ•ˆæœå¾ˆå¥½ã€‚

- å®ƒä»¬è®¤ä¸ºï¼Œåœ¨é¢„è®­ç»ƒæœŸé—´ï¼Œå¯ä»¥ä»æ•°æ®ä¸­å­¦ä¹ å¤šå°ºåº¦å…ˆéªŒçŸ¥è¯†ï¼Œä»è€Œæœ‰å¯èƒ½æ”¾å¼ƒå±‚æ¬¡ç»“æ„

SBä»¬è¯´åˆ†å±‚ä¸å¥½ï¼Œä¿ºåè¦è¯´åˆ†å±‚ç»“æ„æ‰æ˜¯æœ€æ£’å“’ï¼ğŸ˜

è€è§„çŸ©ï¼Œæ§åˆ¶å˜é‡æ³•ï¼šé‡‡ç”¨ç›¸åŒçš„MIMä¸è®­ç»ƒå’Œå¾®è°ƒç¨‹åºæ¥è§‚å¯Ÿplain ViTåœ¨RSä¸Šçš„å½±å“

- å¦‚ä½•åˆ©ç”¨ä¸°å¯Œçš„æœªæ ‡è®°RSIè¿›è¡Œé¢„è®­ç»ƒæ˜¯RSç•Œä¸€ä¸ªéš¾é¢˜

- ç°åœ¨æå‡ºäº†ä¸€äº›è‡ªç›‘ç£å­¦ä¹ SSLæ–¹æ³•ï¼Œä½†æ˜¯ä»–ä»¬ä»…é’ˆå¯¹CNNï¼Œè¿˜æ— æ³•å¯¹å¤§è§„æ¨¡çš„ViTæœ‰æ•ˆ

- ç°åœ¨ä¸€ä¸ªæ–°çš„SSLæ–¹æ³•(MAE)è¢«è¯æ˜å¯¹é¢„è®­ç»ƒplain ViTæœ‰æ•ˆï¼Œä½¿ç”¨ä¸‹æ¸¸ä»»åŠ¡

æœ¬æ–‡ä¸­ï¼Œä½¿ç”¨MAEåœ¨MillionAIDæ•°æ®é›†ä¸Šé¢„è®­ç»ƒplain ViTå’Œæœ€è¿‘æå‡ºçš„ViTAE transformerï¼Œå‚æ•°æœ‰100Mï¼Œæ— æ ‡ç­¾

é¢„è®­ç»ƒåè¦å¯¹æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œè®©ViTé€‚åº”ä¸‹æ¸¸ä»»åŠ¡ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸ºäº†å‡å°‘è®¡ç®—æˆæœ¬å’Œå†…å­˜å ç”¨ï¼Œç›´æ¥æƒ³åˆ°ç”¨å±€éƒ¨çª—å£æ³¨æ„åŠ›ä»£æ›¿å®Œå…¨çš„è‡ªæˆ‘æ³¨æ„åŠ›ğŸ˜

- æŠŠfull self-attentionæ›¿æ¢ä¸ºlocal window attention

- çª—å£çš„å¤§å°å’Œä½ç½®å›ºå®šï¼Œè¿™ä¼šé™åˆ¶åŒºåŸŸæå–æœ‰ç”¨çš„ä¸Šä¸‹æ–‡ï¼Œé™åˆ¶æ¨¡å‹è¡¨ç¤ºèƒ½åŠ›

- æ¥ç€æŸ¥ï¼Œæ‰¾åˆ°ä¸€ä¸ªVSA(varied-size attention)å¯å˜å¤§å°æ³¨æ„åŠ›ï¼Œå¯ä»¥é€šè¿‡å­¦ä¹ å¯è®­ç»ƒçš„ç¼©æ”¾å› å­å’Œåç§»é‡æ¥é€‚åº”çª—å£å¤§å°ã€å½¢çŠ¶å’Œä½ç½®ï¼Œæ¥é€‚åº”ä¸åŒçš„å›¾åƒå†…å®¹ï¼Œåœ¨è®¸å¤šä»»åŠ¡ä¸­æä¾›æ›´å¥½çš„æ€§èƒ½

- åŠ å…¥åœºæ™¯åˆ›æ–°æ³•ï¼šå¯¹VSAæ‰©å±•åˆ°æ—‹è½¬å¯å˜å¤§å°æ³¨æ„åŠ›(RVSA)ï¼Œè¿™å°±æ˜¯ä¿ºä»¬æ»´ä¸€ä¸ªåˆ›æ–°ç‚¹å“’

æœ€åæŠŠä¸Šé¢ä¸¤ä¸ªä¸œè¥¿ä½¿ç”¨plain ViTå’ŒViTAEæ¨¡å‹æ¥æ£€æµ‹æ€§èƒ½ï¼Œæ•ˆæœä¸é”™ï¼ŒSOTAğŸ‘
