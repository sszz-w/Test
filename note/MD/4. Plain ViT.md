# Advancing Plain Vision Transformer Towards Remote Sensing Foundation Model

> Di Wang

Large-scale vision foundation models have made significant progress in visual tasks on natural images, where the vision transformers are the primary chocie for their good scalability and representation ability.

However, the utilization of large models in the remote sensing (RS) community remains under-explored where existing models are still at small-scale, which limits the performance.

In this paper, we resort to plain vision transformers with about 100 million parameters and make the first attempt to propose large vision models customized for RS tasks and explore how such large models perform.

Specifically, to handle the large image size and objects of various orientations in RS images, we propose a new rotated varied-size window attention to substitute the original full attention in transformers, which could significantly reduce the computational cost and memory footprint while learn better object representation by extracting rich context from the generated diverse windows.

Experiments on detection tasks demonstrate the superiority of our model over all state-of-the-art models, achieving 81.16% mAP on the DOTA-V1.0 dataset.

The geninformatics community is vigoriously developing by combining with computer technologies because of the necessity for human production, and the most representative achievement is the "4D product" including digital Line graphic, digital elevation model, digital raster graphic, and digital orthophoto map (DOM). Among these productions, the remote sensing image (RSI) is the dominant data source for generating the DOM since it is easy to access and can be obtained in real-time.

The RSIs are being employed in many valuable applications such as scene recognition for land use and land cover classification for precision agriculture and object detection for matitime monitoring.

However, existing studies reveal that the limited receptive field of convolution in each layer makes it difficult for CNNs to pay attention to long-range pixels and extract global context.

To address this issue, the self-attention (SA) mechanism is proposed to obtain flexible global dependency by enabling the interaction between arbitrary pixels in images, delivering promising results in the computer vision (CV) field.

Further, vision transformer adopts the design of multi-head SA (MHSA), which simulataneously implements the above provedure in multiple projected subspaces, which diversifies the extracted contexts and improves the feature representation.

The plain vision transformer (ViT) is a very simple architecture that stacks several transformer encoder blocks sequentially after the patch embedding layer, where features after each block are at the same scale.

Nevertheless, due to the strong presentation ability of the transformers, researchers question the necessity of the hierarchial design in transformers for downstream tasks.

Thanks to the development of the unsupervised learning in masked image modeling (MIM), recent work reveals that such a pretraining processs can give a good initialization for plain ViTs to achieve surprising results on various downstream tasks including object detection and pose estimation.

The main insight behine their success is that the multiscale prior can be learned from the data during the pretraining, thus making it possible to discard the hierarchical structure.

A recently proposed method named varied-size attention (VSA) addresses this issue by learning trainable scaling factors and offset to adapt the size, shape and location of windows to diverse image content, thus delivering better performance on many tasks.

(1) We demonstrate the possibility of pretraining plain ViTs with about 100 million parameters on RSIs, adapting them for downstream RS tasks, and delivering competitive performance. To our best knowledge, they are so far the largest models in the remote sening community, making a step towards the remote sensing foundation model.

(2) We introduce a learnable rotation mechanism into the vision transformer to learn varied-size windows with different orientation angles for attention calculation, which is very suitable to deal with RSIs. It promotes the extraction of rich context from the generated windows and learning better feature representation.

(3) Experimental results show that the proposed models set new state-of-the-art (SOTA) on the detection task, and obtain competitive performances on classification and segmentation tasks. Besides, we also show the advantanges of the proposed models in terms of computational complexity and few-shot learning ability.

--------------

Plain Vision Transformer中使用具有约1亿个参数🤦‍♀️

Plain Vision Transformer提出了一种新的旋转可变大小窗口注意rotated variedsize window attention，来对原来的transformer中的original full attention进行替换，显著降低计算成本和内存占用；同时通过从生成的不同窗口中提取丰富的上下文来学习更好的对象表示

|Model|mAP|Conference|Recommond|
|--|--|--|--|
|ViT+RVSA-ORCN|81.16|-|-|

--------------

RSI = remote sensing image

当前，卷积神经网络（CNN）是提取分层多尺度视觉特征最常用的模型，每层卷积的有限感受域使得CNN很难关注远距离像素并提取全局上下文。

为了解决上面那个问题，提出了self-attention SA 机制实现图像中任意像素之间的交互来获得灵活的全局依赖性

因此，transformer 采用了多头SA（MHSA）的设计，该设计在多个投影子空间中同时实现了上述过程，从而使提取的上下文多样化，并改进了特征表示。👍

Plain Vision Transformer = ViT 在patch embedding层之后顺序堆叠多个transformer编码器块，其中每个块之后的特征具有相同的比例

借鉴了CNN中的分层设计思想，并相应地设计了分层ViT。这些模型通常使用大规模数据集以有监督的方式进行预训练，并对下游任务进行微调

最近的论文有论证，分层ViT相对于CNN的优越性。而transformer有强大的表示能力，那么下游任务中分层transformer的设计是不是必要的？😒

- 根据最近的论文MiM(Masked image modeling)中无监督学习的发展，这种与训练过程可以为plain ViT提供很好的初始化，导致下游任务的效果很好。

- 它们认为，在预训练期间，可以从数据中学习多尺度先验知识，从而有可能放弃层次结构

SB们说分层不好，俺偏要说分层结构才是最棒哒！😍

老规矩，控制变量法：采用相同的MIM与训练和微调程序来观察plain ViT在RS上的影响

- 如何利用丰富的未标记RSI进行预训练是RS界一个难题

- 现在提出了一些自监督学习SSL方法，但是他们仅针对CNN，还无法对大规模的ViT有效

- 现在一个新的SSL方法(MAE)被证明对预训练plain ViT有效，使用下游任务

本文中，使用MAE在MillionAID数据集上预训练plain ViT和最近提出的ViTAE transformer，参数有100M，无标签

预训练后要对数据集进行微调，让ViT适应下游任务。这个过程为了减少计算成本和内存占用，直接想到用局部窗口注意力代替完全的自我注意力😎

- 把full self-attention替换为local window attention

- 窗口的大小和位置固定，这会限制区域提取有用的上下文，限制模型表示能力

- 接着查，找到一个VSA(varied-size attention)可变大小注意力，可以通过学习可训练的缩放因子和偏移量来适应窗口大小、形状和位置，来适应不同的图像内容，在许多任务中提供更好的性能

- 加入场景创新法：对VSA扩展到旋转可变大小注意力(RVSA)，这就是俺们滴一个创新点哒

最后把上面两个东西使用plain ViT和ViTAE模型来检测性能，效果不错，SOTA👍
