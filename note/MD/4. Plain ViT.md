# Advancing Plain Vision Transformer Towards Remote Sensing Foundation Model

> Di Wang

Plain Vision Transformer中使用具有约1亿个参数🤦‍♀️

Plain Vision Transformer提出了一种新的旋转可变大小窗口注意rotated variedsize window attention，来对原来的transformer中的original full attention进行替换，显著降低计算成本和内存占用；同时通过从生成的不同窗口中提取丰富的上下文来学习更好的对象表示

|Model|mAP|Conference|Recommond|
|--|--|--|--|
|ViT+RVSA-ORCN|81.16|-|-|

--------------

RSI = remote sensing image

当前，卷积神经网络（CNN）是提取分层多尺度视觉特征最常用的模型，每层卷积的有限感受域使得CNN很难关注远距离像素并提取全局上下文。

为了解决上面那个问题，提出了self-attention SA 机制实现图像中任意像素之间的交互来获得灵活的全局依赖性

因此，transformer 采用了多头SA（MHSA）的设计，该设计在多个投影子空间中同时实现了上述过程，从而使提取的上下文多样化，并改进了特征表示。👍

Plain Vision Transformer = ViT 在patch embedding层之后顺序堆叠多个transformer编码器块，其中每个块之后的特征具有相同的比例

借鉴了CNN中的分层设计思想，并相应地设计了分层ViT。这些模型通常使用大规模数据集以有监督的方式进行预训练，并对下游任务进行微调

最近的论文有论证，分层ViT相对于CNN的优越性。而transformer有强大的表示能力，那么下游任务中分层transformer的设计是不是必要的？😒

- 根据最近的论文MiM(Masked image modeling)中无监督学习的发展，这种与训练过程可以为plain ViT提供很好的初始化，导致下游任务的效果很好。

- 它们认为，在预训练期间，可以从数据中学习多尺度先验知识，从而有可能放弃层次结构

SB们说分层不好，俺偏要说分层结构才是最棒哒！😍

老规矩，控制变量法：采用相同的MIM与训练和微调程序来观察plain ViT在RS上的影响

- 如何利用丰富的未标记RSI进行预训练是RS界一个难题

- 现在提出了一些自监督学习SSL方法，但是他们仅针对CNN，还无法对大规模的ViT有效

- 现在一个新的SSL方法(MAE)被证明对预训练plain ViT有效，使用下游任务

本文中，使用MAE在MillionAID数据集上预训练plain ViT和最近提出的ViTAE transformer，参数有100M，无标签

预训练后要对数据集进行微调，让ViT适应下游任务。这个过程为了减少计算成本和内存占用，直接想到用局部窗口注意力代替完全的自我注意力😎

- 把full self-attention替换为local window attention

- 窗口的大小和位置固定，这会限制区域提取有用的上下文，限制模型表示能力

- 接着查，找到一个VSA(varied-size attention)可变大小注意力，可以通过学习可训练的缩放因子和偏移量来适应窗口大小、形状和位置，来适应不同的图像内容，在许多任务中提供更好的性能

- 加入场景创新法：对VSA扩展到旋转可变大小注意力(RVSA)，这就是俺们滴一个创新点哒

最后把上面两个东西使用plain ViT和ViTAE模型来检测性能，效果不错，SOTA👍
