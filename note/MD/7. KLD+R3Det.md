# Learning High-Precision Bounding Box for Rotated Object Detection vai Kullback-Leibler Divergence

> Xue Yang, NeurIPS 2021

通过Kullback-Leibler Divergence相对熵来进行Bounding Box的旋转目标检测

|Model|mAP|Conference|Recommond|
|--|--|--|--|
|KLD+$R^3$Det|80.63|NeurIPS2021|:white_check_mark:|

目前的回归损失设计有缺陷，在高精度检测中表现不好，尤其是大纵横比目标

论文动机是根据旋转和水平检测间的关系，将旋转回归损失的设计从归纳范式转变为演绎方法（很牛逼🤩）

估计的参数能在动态联合优化期间以自适应和协同的方式相互影响，如何调制旋转回归损失中的耦合参数？

- 首先将旋转后的边界框转换为二维高斯分布，然后计算高斯分布之间的KLD作为回归损失

- 分析每个参数的梯度：KLD可以根据对象的特征动态调整参数梯度

- 根据纵横比调整角度参数的重要性

证明了KLD是尺度不变的！！！

这篇论文和之前的GWD论文有相似之处，把里面的Wasserestein Distance换成了KLD，但是在理论上也证明了为什么比沃瑟斯坦距离更好

KLD Loss可以退化为用于水平检测的$l_n$ Loss

---

水平检测的Loss想转到旋转领域需要的参数有$x,y,w,h,\theta$，不同的参数对不同类型对象的重要性会不同：$\theta 和 x,y$对大纵横比、小对象很重要

推测回归损失在学习过程中应该是自调制的，需要更多的动态优化策略

- 从头开发新的旋转检测损失，证明它在水平检测的退化情况一致

- 将旋转检测的回归损失转换为KLD两个高斯分布

- KLD的自调制优化机制促进了精度的提升；理论上证明了KLD的尺度不变性
